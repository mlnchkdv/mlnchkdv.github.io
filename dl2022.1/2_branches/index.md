# [⬅ Глубокое обучение 2022.1](../index.html)

## Разделы машинного обучения

[TOC]

<img src=".\images\Слайд1.png" style="zoom:50%;" />

Цель машинного обучения — предсказать результат по входным данным. Чем разнообразнее входные данные, тем проще алгоритму (машине, системе и т.п.) **найти закономерности** и сгенерировать результат.

Классифицировать алгоритмы можно многими способами. И начнем с обзора  четырех основных направлений в машинном обучении на сегодняшний день.


![](.\images\7s8.jpg)

Как мы видим из данной схемы, можно выделить три составляющих:

- Данные

  > От качества, объема и типа данных зависит в итоге как будет сформулирована задача и подобраны методы её решения. Чем качественнее данные, тем эффективнее будет работать программа. 

- Признаки

  > Также их называют фичами (*features*), свойствами, характеристиками — то на что конкретно необходимо смотреть алгоритму в подаваемых ему данных. Признаками могут быть пробег автомобиля, пол пользователя, цена акций, даже счетчик частоты появления слова в тексте или даже просто подпись к фотографии кота, что это есть кот.

- Алгоритм

  > Как правило одну и туже задачу можно решать по разному. От выбора алгоритма зависит точность, скорость работы и размер готовой модели. Поэтому  сначала необходимо, сформулировать и определить критерии и меры оценки эффективности работы выбранных алгоритмов.

Таким образом определение машинного обучение можно вывести исходя из наглядной диаграммы:

![](.\images\7r8.jpg)

Машинное обучение — направление, связанное с разработкой и построением аналитических моделей, которые способны автоматически обнаружить в данных скрытые и ранее неизвестные закономерности, а также самостоятельно приобретать новые свойства, необходимые для распознавания этих закономерностей. Или проще говоря, машинное обучение — подход при котором  наша программа, сама строит алгоритм (модель, функцию) из некого первоначального (исходного) набора данных.

Ниже  представлен развернутый вариант условной классификации машинного обучения.

![](.\images\карта.jpg)

Классификация является условной и отображение разделов и подразделов могут быть представлены в других вариантах и формах.

![](.\images\_1.jpg)

![](.\images\DhMOKAXX4AAXvMB.jpg)

![](.\images\E0oxqlqUcAQ_SJM.jpg)

![](.\images\Eq6Ecl2XEAAfp-k.png)

![](.\images\Machine Learning tasks, classification, slustering and Dimentionality redirection.png)

![](.\images\ML-DL-Shema.jpg)

![](.\images\qv8tkjpyrk_qeia-hp4fxkvco7w.jpeg)

![](.\images\scale_1200.jpg)

![](.\images\DH2a9frUQAAG7JV.jpg)

![](.\images\maxresdefault.jpg)

В некоторых источниках можно встретить еще выражение **продвинутые методы машинного обучения**, к которым относят методы EDA, классическое машинное обучение, алгоритмы на графах и ансамбли моделей.



### Классическое машинное обучение

Первые алгоритмы, заложившие классическое машинное обучение, пришли к нам из статистики и теории вероятности еще в 1950-х годах. Они решали формальные задачи — искали закономерности в данных (как правило в цифровых), оценивали близость точек в пространстве и вычисляли направления.

![](.\images\7r6.jpg)

> Стоит отметить, что на сегодняшний день, крупные корпорации любят использовать нейронные сети везде, где это возможно. Потому что лишние 2% точности для них легко конвертируются в дополнительные прибыли и минимизацию рисков. Но остальным же стоит помнить, что когда задача решается классическими методами, дешевле реализовать сколько-нибудь полезное и эффективное решение на них, а потом думать об улучшениях. 



#### Обучение с учителем

Классическое машинное обучение в канонической интерпретации делится на две категории — с учителем (*Supervised Learning*) и без учителя (*Unsupervised Learning*).

В первом случае у алгоритма есть некий учитель, который говорит ей как правильно. Рассказывает, что на этой картинке кошка, а на этой собака. То есть учитель уже заранее разделил (разметил) все данные на кошек и собак, а алгоритм учится на конкретных примерах.

В обучении без учителя, алгоритм просто получает массив никак не размеченных данных, к примеру архив фотографий животных и требуется «разобраться, кто здесь на кого похож». Данные не размечены, у алгоритма нет учителя, и он пытается сам найти любые закономерности.

Очевидно, что с учителем обучаемость быстрее и точнее, поэтому напрактике его используют намного чаще. В свою очередь эти задачи делятся на два типа: **классификация — предсказание категории объекта, и регрессия — предсказание места на числовой прямой**.



##### Классификация

![](.\images\7qx.jpg)

*«Разделяет объекты по заранее известному признаку на заранее известные классы. Глаза по цветам, документы по языкам, музыку по жанрам и т.д.»*

Сегодня используют для:

- Спам-фильтры
- Определение языка
- Поиск похожих документов
- Анализ тональности
- Распознавание рукописных букв и цифр
- Определение подозрительных транзакций

Популярные алгоритмы: [Наивный Байес](https://ru.wikipedia.org/wiki/Наивный_байесовский_классификатор), [Деревья Решений](https://logic.pdmi.ras.ru/~sergey/teaching/mlcsclub/02-dectrees.pdf), [Логистическая Регрессия](https://ru.wikipedia.org/wiki/Логистическая_регрессия), [K-ближайших соседей](https://ru.wikipedia.org/wiki/Метод_k-ближайших_соседей), [Машины Опорных Векторов](https://ru.wikipedia.org/wiki/Метод_опорных_векторов)



ДОБАВИТЬ



Также классификация используется для поиска аномалий в данных, т.е. когда один или несколько признаков объекта сильно выделяются или проще говоря не вписываются в наши классы. К примеру, в медицине сразу подсвечиваются все подозрительные области МРТ или выделяет отклонения в анализах. На биржах таким же образом определяют нестандартных игроков, которые скорее всего являются инсайдерами. Научив компьютер «как правильно», мы автоматически получаем и обратный классификатор — как неправильно.

Сегодня для классификации всё чаще используют нейросети, ведь по сути их для этого и изобрели. Но не стоит забывать, что **чем сложнее данные — тем сложнее алгоритм**. Для текста, цифр, таблиц разумнее начинать с классических методов машинного обучения, где модели меньше, обучаются быстрее и работают понятнее. Для изображений, видео и всего что называется Big Data — разумнее начинать с нейросетей.

![](.\images\MFCC+…+……+spectrogram+DFT+Waveform+DCT+log+Input+of+DNN+filter+bank.jpg)

Подходы решения типовых задач, не стоят на месте, так одним из ярких примером является, задача распознания речи (которая является комбинированной задачей) решалась преобразованием звукового сигнала в MFCC-признаки (*mel-frequency cepstral coefficients*) (т.е. бралось преобразование Фурье, получившийся спектр преобразовывался к логарифмам амплитуд частот и далее от этих логарифмов бралось обратное преобразование Фурье) и уже получившиеся фонемы распознавались с помощью скрытых марковских моделей (т.е. оценивалась вероятность следующего слова при условии нескольких предыдущих). С приходом нейронных сетей, данный подход (основанный на скрытых марковских моделях) был заменен, более того, оказалось, что MFCC-признаки тоже можно улучшить путем обучения (естественно процесс предобработки сигнала остался) и в результате, сигналы (ваш голос) подающиеся на вход современным системам распознавания (такие как Siri, Алиса и т.п.), стал гораздо более «сырым», чем MFCC и при этом более эффективным (см. ниже про рекуррентные нейронные сети). Еще пример, не так давно можно было встретить классификатор лиц на SVM, но сегодня под эту задачу существует множество готовых нейросетей (см. ниже про свёрточные нейронные сети) в открытом доступе. А вот спам-фильтры, начиная с 2010 года, реализуются на основе SVM метода, так и используются по сей день. 



##### Регрессия

![](.\images\7qy.jpg)

*«Нарисуй линию вдоль моих точек. Да, это тоже машинное обучение»*

Сегодня используют для:

- Прогноза стоимости ценных бумаг
- Анализ спроса и объема продаж
- Медицинские диагнозы
- Любые зависимости числа от времени (временные ряды)
- Эконометрика

Популярные алгоритмы: [Линейная или Полиномиальная Регрессия](http://www.machinelearning.ru/wiki/index.php?title=Линейная_регрессия_(пример))

Регрессия — та же классификация, только вместо категории мы предсказываем число (линейная аппроксимация). Стоимость автомобиля по его пробегу, количество пробок по времени суток, объем спроса на товар от роста компании и т.д. На регрессию идеально ложатся любые задачи, где есть зависимость от времени.

Регрессия является базовым инструментом у финансистов и аналитиков, она  также встроена в *Excel*, *Tableau*, *Power BI* и есть во всех статистических и аналитических библиотеках. Базовый принцип работы алгоритма — попытаться нарисовать линию, которая в среднем отражает зависимость и в отличие от человека с фломастером и бумагой, делается это математически точно — рассчитывая и учитывая среднее расстояние до каждой точки.

![](.\images\7rg.jpg)

Когда регрессия рисует прямую линию, её называют линейной, когда кривую — полиномиальной. Это два основных вида регрессии, остальные встречаются реже. Также не стоит забывать, что Логистическая Регрессия, это не регрессия, а метод классификации.

Схожесть регрессии и классификации подтверждается еще и тем, что многие классификаторы, после небольших преобразований, превращаются в регрессоры. Например, мы можем не просто смотреть к какому классу принадлежит объект, а запоминать, насколько он близок — и вот регрессия.

Также регрессию и её подразделы можно классифицировать как методы прогнозирования — поиск конкретного числа, которое ожидается получить для нового наблюдения или для будущих периодов. В их числе [регрессия, оцененная методом наименьших квадратов](http://www.cleverstudents.ru/articles/mnk.html) для оценки зависимости одного фактора от другого. Модель временных рядов [ARIMA](https://zen.yandex.ru/media/id/5fd12882382a85570c79c48c/arima-v-mashinnom-obuchenii-prostymi-slovami-6191307a9380933c070663a2) для создания прогнозов (прогностических моделей), где *auto-regression* — зависимость от значений в прошлом периоде, *integrated* — процесс избавления от неcтационарности (т.е. наличие тренда или цикличности) и *moving average*  — зависимость от остатка  (разница между текущим и предыдущим значением) в прошлом периоде. [GARCH](https://www.machinelearningmastery.ru/develop-arch-and-garch-models-for-time-series-forecasting-in-python/) (*general auto-regression conditional heteroscedastic*) модель — применяется, когда во временных рядах есть гетероскедастичность (так называемая кластеризация). И множество модификаций под разные виды временных рядов.

> Конспект по методам прогнозирования: https://habr.com/ru/post/493396/



#### Обучение без учителя

Обучение без учителя (*Unsupervised Learning*) было изобретено в 90-е годы, и на практике используется реже. Поскольку **размеченные данные — дорогая редкость**, то обучение без учителя и используется для того, чтобы автоматически подписывать (размечать) исходные (сырые) данные.

> Когда обучение без учителя не даёт удовлетворительного результата, а такое случается довольно часто, то на помощь приходят такие сервисы как [Яндекс.Толока](https://toloka.yandex.ru/), которые в ручную, руками своих пользователей, размечают данные, за небольшие деньги. Именно так Яндекс и другие крупные корпорации и создают свои датасеты.

Обучение без учителя, чаще используют как метод анализа данных, а не как основной алгоритм решения задачи машинного обучения и является частью интеллектуального анализа данных (*Data Mining*), т.е. обнаружения в больших массивах данных, ранее неизвестных, практически полезных и доступных для интерпретации знаний, необходимых для принятия решений.



##### Кластеризация

![](.\images\7qz.jpg)

*«Разделяет объекты по неизвестному признаку. Алгоритм сам решает как ему лучше»*

Сегодня используют для:

- Сегментация рынка (типов покупателей, лояльности)
- Поиск ключевых игроков, «китов» рынка
- Объединение близких точек на карте
- Сжатие изображений
- Анализ и разметки новых данных
- Детекторы аномального поведения

Популярные алгоритмы: [Метод K-средних](https://en.wikipedia.org/wiki/K-means_clustering), [Mean-Shift](https://en.wikipedia.org/wiki/Mean_shift), [DBSCAN](https://en.wikipedia.org/wiki/DBSCAN)

5 алгоритмов кластеризации: [The 5 Clustering Algorithms Data Scientists Need to Know](https://towardsdatascience.com/the-5-clustering-algorithms-data-scientists-need-to-know-a36d136ef68)

![](.\images\7ri.jpg)

Кластеризация — это классификация, но без заранее известных классов. Она сама ищет похожие объекты и объединяет их в кластеры. Количество кластеров можно задать заранее или доверить это алгоритму. Похожесть объектов определяется по тем признакам, которые разметили заранее — у кого много схожих характеристик, тех объединяем в один класс.

Отличный пример кластеризации — маркеры на картах, в таких сервисах, как Циан или доставка еды. Когда вы ищете все 3-х комнатные квартиры или все грузинские рестораны в Москве, то сервису приходится группировать их в кружочки с числом (кластер) и при увеличении распадаться на множество более мелких, с привязкой к конкретному адресу, иначе браузер или приложение, просто бы зависло в попытке загрузить и отрисовать миллион маркеров на карте.

Более сложные примеры кластеризации можно вспомнить в приложениях iPhoto или Google Photos, которые находят лица людей на фотографиях и группируют их в альбомы. Приложение не знает как зовут ваших друзей, но может отличить их по характерным чертам лица. Типичная кластеризация. Правда для начала им приходится найти эти самые «характерные черты», а это уже только с учителем.

Сжатие изображений — еще одна популярная проблема. Сохраняя картинку в PNG, вы можете установить палитру, скажем, в 32 цвета. Тогда кластеризация найдёт все «примерно красные» пиксели изображения, высчитает из них «средний красный» и заменит все красные на этот цвет, поскольку меньше цветов — меньше файл.

Проблема только, как быть с цветами типа Cyan — он ближе к зеленому или синему? В таком случае используется популярный алгоритм кластеризации — [Метод К-средних (K-Means)](https://www.youtube.com/watch?v=_aWzGGNrcic). Где случайным образом бросается на палитру цветов наши 32 точки, называя их центроидами. Все остальные точки относятся к ближайшему центроиду от них — получаются как бы созвездия из самых близких цветов. Затем центроид двигается в центр своего созвездия и повторяет действие пока центроиды не перестанут двигаться. В результате кластеры обнаружены, стабильны и их ровно 32.

Конечно искать центроиды удобно и просто, но в реальных задачах кластеры могут быть разных форм. Например в геологии, необходимо найти на карте схожие по структуре горные породы — т.е. кластеры не только будут вложены друг в друга, но и нет точного их количества. Одним из решений будет метод [DBSCAN](https://habr.com/post/322034/). Данный метод сам находит скопления точек и строит вокруг них кластеры. Его легко визуализировать и понять, если представить, что точки — это люди на площади. Находим трёх любых близко стоящих человека и говорим им взяться за руки. Затем они начинают брать за руку тех, до кого могут дотянуться. Так по цепочке, пока никто больше не сможет взять кого-то за руку — это и будет первый кластер. Повторяем, пока не поделим всех. Те, кому вообще некого брать за руку — это выбросы и аномалии.

Как и классификация, кластеризация тоже может использоваться как детектор аномалий. Скажем необходимо ответить на вопрос, отличается ли поведение пользователя после регистрации от нормального? При этом даже не требуется знать, что есть «нормальное поведение» — на вход просто загружаются все действия пользователей в алгоритм (модель), а далее нормальность и аномалии сами детектируются. Но стоит отметить, что работает такой подход, по сравнению с классификацией, как правило, менее эффективно.



##### Уменьшение размерности (обобщение)

![](.\images\7r0.jpg)

*«Собирает конкретные признаки в абстракции более высокого уровня»*

Сегодня используют для:

- Рекомендательные Системы (★)
- Визуализации данных
- Определение тематики и поиска похожих документов
- [Анализ фейковых изображений](https://vas3k.ru/blog/390/)
- Риск-менеджмент

Популярные алгоритмы: [Метод главных компонент](https://ru.wikipedia.org/wiki/Метод_главных_компонент) (PCA), [Сингулярное разложение](https://ru.wikipedia.org/wiki/Сингулярное_разложение) (SVD), [Латентное размещение Дирихле](https://ru.wikipedia.org/wiki/Латентное_размещение_Дирихле) (LDA), [Латентно-семантический анализ](https://ru.wikipedia.org/wiki/Вероятностный_латентно-семантический_анализ) (LSA, pLSA, GLSA), [t-SNE](https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding) (для визуализации)

![](.\images\7rv.jpg)

С ростом объемов и сложности исследуемых данных, появилась необходимость в уменьшении и упрощении для дальнейшей интерпретации и анализа (т.е. новый уровень абстракции) исходных данных и данный класс методов назвали уменьшение (понижение) размерности или обобщение (*Dimension Reduction* или *Feature Learning*).

Например, собаки с треугольными ушами, длинными носами и большими хвостами объединяются в абстракцию «овчарки». Да, мы теряем информацию о конкретных овчарках, но новая абстракция становится удобнее и полезнее для анализа и интерпретации данных, без лишних деталей. Иными словами мы объединили несколько признаков в один и получили абстракцию. Плюс, обучение на меньшем количестве размерностей работает быстрее.

Данный инструмент эффективно используется для определения тематик текстов (*Topic Modelling*). Так метод [Латентно-семантического анализа](https://habr.com/post/110078/) (LSA), рассчитывает тематику текста из частоты появления определенных слов данном в тексте: в научных статьях больше технических терминов, в новостях о политике — имён политиков. Иными словами, происходит абстрагирование от конкретных слов до уровня смыслов, даже без привлечения учителя со списком категорий. Да, мы могли бы просто взять все слова из статей и кластеризовать, но тогда бы были потеряны все полезные связи между словами, например, что *батарейка* и *аккумулятор*, означают одно и то же в разных документах. Но точность такой системы будет крайне низкой, поскольку, нужно как-то объединить слова и документы в один признак, чтобы не терять скрытые (латентные) связи. Отсюда и появилось название метода. Оказалось, что [Сингулярное разложение](https://ru.wikipedia.org/wiki/Сингулярное_разложение) (SVD) легко справляется с этой задачей, выявляя полезные тематические кластеры из слов, которые встречаются вместе.

> Смотри также статью [Как уменьшить количество измерений и извлечь из этого пользу](https://habr.com/post/275273/), и практическое применение в статье [Алгоритм LSA для поиска похожих документов](https://netpeak.net/ru/blog/algoritm-lsa-dlya-poiska-pohozhih-dokumentov/).

Другое популярное применение метода уменьшения размерности нашли в рекомендательных системах и коллаборативной фильтрации. Оказалось, если абстрагировать ими оценки пользователей фильмам, получается неплохая система рекомендаций кино, музыки, игр и т.д.

Полученная абстракция может быть с трудом понимаема мозгом, но при более тщательном исследовании и анализе, могут быть найдены новые признаки и корреляции, неочевидные ранее. Алгоритм, не знавший ничего кроме оценок пользователей, может совершенно иначе оперировать с ними и выдавать неожиданные связи.

> Лекция Яндекса — [Как работают рекомендательные системы](https://habr.com/company/yandex/blog/241455/)

![](.\images\Screen-Shot-2020-01-07-at-2.56.03-PM-e1578388354711.png)

Также уменьшение размерности используется в целях визуализации, перед тем, как работать с многомерными данными, может быть полезно посмотреть на их структуру, уменьшив размерность и спроецировав их на двумерную или трехмерную плоскость. 



##### Поиск правил (ассоциация)

![](.\images\7r1.jpg)

*«Ищет закономерности в потоке заказов»*

Сегодня используют для:

- Прогноз акций и распродаж
- Анализ товаров, покупаемых вместе
- Расстановка товаров на полках
- Анализ паттернов поведения на веб-сайтах

Популярные алгоритмы: [Apriori, Euclat, FP-growth](https://en.wikipedia.org/wiki/Association_rule_learning#Algorithms)

К поиску правил (ассоциаций) относятся все методы анализа продуктовых корзин, стратегий маркетинга и других последовательностей.



### Обучение с подкреплением

*«Помести робота в лабиринт и пусть ищет выход»*

Сегодня используют для:

- Самоуправляемых автомобилей
- Роботов пылесосов
- Игр
- Автоматической торговли
- Управления ресурсами предприятий

Популярные алгоритмы: [Q-Learning](https://ru.wikipedia.org/wiki/Q-обучение), [SARSA](https://en.wikipedia.org/wiki/State–action–reward–state–action), DQN, [A3C](https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-8-asynchronous-actor-critic-agents-a3c-c88f72a5e9f2), [Генетический Алгоритм](https://ru.wikipedia.org/wiki/Генетический_алгоритм)

![]()



Термин подкрепление (reinforcement) пришёл из поведенческой психологии и обозначает награду или наказание за некоторый получившийся результат, зависящий не только от самих принятых решений, но и внешних, не обязательно подконтрольных, факторов. Под обучением здесь понимается поиск способов достичь желаемого результата методом проб и ошибок (trial and error), то есть попыток решить задачу и использование накопленного опыта для усовершенствования своей стратегии в будущем.



Обучение с подкреплением используют там, где задача стоит не анализ данных, а «выживание» в реальной среде. Средой может быть как реальный мир, например — автопилот Теслы или роботы-пылесосы, так и в качестве  среды может быть видеоигра, как пример, NPC в некоторых компьютерных играх.

> https://youtu.be/qv6UVOQ0F44 - пример нейросети играющей в Марио







### Ансамбли

#### Стекинг

про стеккинг неправильно написано. то, что там написано - это разновидность бэггинга. проверьте плиз

#### Беггинг

#### Бустинг

### Нейронные сети и глубокое обучение

![](.\images\7r4.jpg)

*«У нас есть сеть из тысячи слоёв, десятки видеокарт и терабайты данных. Пусть рисует котиков!»*

Сегодня используют для:

- Вместо всех вышеперечисленных алгоритмов вообще
- Определение объектов на фото и видео
- Распознавание и синтез речи
- Обработка изображений, перенос стиля
- Машинный перевод

Популярные архитектуры: [Перцептрон](https://ru.wikipedia.org/wiki/Перцептрон), [Свёрточные Сети](https://ru.wikipedia.org/wiki/Свёрточная_нейронная_сеть) (CNN), [Рекуррентные Сети](https://ru.wikipedia.org/wiki/Рекуррентная_нейронная_сеть) (RNN), [Автоэнкодеры](https://ru.wikipedia.org/wiki/Автокодировщик)

![](.\images\7sb.jpg)

Любая нейросеть — это **набор нейронов и связей между ними**. Нейрон  представляет из себя просто функцию со множеством входов и одним выходом. Задача нейрона — взять числовые значения со своих входов, выполнить над ними функцию и отдать результат на выход. Простой пример нейрона: просуммировать все цифры со входов, и если их сумма больше N — выдать на выход единицу, иначе — ноль. В свою очередь, связи — это каналы, через которые нейроны шлют друг другу какие-то числовые значения. У каждой связи есть свой вес — её единственный параметр, который можно условно представить как прочность (качество, стоимость) связи. Когда через связь с весом 0.5 проходит число 10, оно превращается в 5. Сам нейрон не разбирается, что к нему пришло и суммирует всё подряд — вот веса и нужны, чтобы управлять на какие входы нейрон должен реагировать, а на какие нет.

![](.\images\7se.jpg)

Далее, чтобы управлять сетью, нейроны решили связывать не как захочется, а по слоям. **Внутри одного слоя нейроны никак не связаны**, но соединены с нейронами следующего и предыдущего слоя. Данные в такой сети идут строго в одном направлении — от входов первого слоя к выходам последнего. Если объединить достаточное количество слоёв и правильно расставить веса в такой сети, получается следующее — подав на вход, скажем, изображение написанной от руки цифры 4, чёрные пиксели активируют связанные с ними нейроны, те активируют следующие слои, и так далее и далее, пока в итоге не загорится самый выход, отвечающий за четвёрку. Результат достигнут.

С точки зрения машинного обучения, нейронная сеть — частный случай методов распознавания образов и дискриминантного анализа (раздел вычислительной математики о том, какие переменные разделяют, то есть «дискриминируют», данные на известные заранее (в отличие от кластерного анализа) страты). На практике, естественно, никаких нейронов и связей не пишут, всё представляют матрицами и считают матричными произведениями, для эффективной скорости. 

> Наглядное [объяснение](https://youtu.be/aircAruvnKk) распознавания рукописных цифр.

Такая сеть, где несколько слоёв и между ними связаны все нейроны, называется [перцептроном](https://ru.wikipedia.org/wiki/Многослойный_перцептрон_Румельхарта) (MLP) и считается самой простой архитектурой для новичков. Как правило, в реальных проектах не используется, в виду трудоемкости её обучения и низкой эффективности.

Таким образом основная задача, когда мы построили сеть, это правильно расставить веса, чтобы нейроны реагировали на нужные сигналы. Тут нужно вспомнить, что у нас же есть данные — примеры «входов» и правильных «выходов». Будем показывать нейросети рисунок той же цифры 4 и говорить «подстрой свои веса так, чтобы на твоём выходе при таком входе всегда загоралась четвёрка».

В начале обучения все веса расставлены случайно, мы показываем сети цифру, она выдаёт какой-то случайный ответ (весов-то нет), а мы сравниваем, насколько результат отличается от нужного нам. Затем идём по сети в обратном направлении, от выходов ко входам, и для каждого нейрона, между выходным и входным слоями, указываем, через веса, правильно или неправильно был активирован нейрон (проще говоря, спрашиваем у каждого нейрона — так, ты вот тут зачем-то активировался, из-за тебя всё пошло не так, давай ты будешь чуть меньше реагировать на вот эту связь и чуть больше на вон ту).

В результате, через сто тысяч таких циклов «прогнали-проверили-исправили» веса в сети откорректируются (с большой долей вероятности) так, как мы хотели. Этот подход называется [*Backpropagation*](https://en.wikipedia.org/wiki/Backpropagation) или «Метод обратного распространения ошибки». С виду очевидный и понятный метод, был открыт еще в 60-е годах прошлого века, но потребовалось 30 лет, чтобы применить его к нейронным сетям (Румельхарт, Хинтон и Уильямс в статье под названием [«*Learning representations by back-propagating errors*»](https://www.nature.com/articles/323533a0)). До этого не существовало универсального подхода для их обучения.

> Наглядное [объяснение](https://youtu.be/IHZwWFHWa-w) процесса Backpropagation.

![](.\images\3-AI-Booms.png)

Итак, хорошо обученная нейросеть может «притворяться» любым алгоритмом из представленных выше. Появилась иллюзия, что *наконец-то у нас есть архитектура человеческого мозга, нужно просто собрать много слоёв и обучить их на любых данных*. Но оказалось, что на обучение сети с большим количеством слоёв требуется невозможный объем мощностей, по тем временам (на сегодняшний день, средний игровой компьютер с GeForce превышает мощность тогдашнего датацентра), и началась первая [Зима ИИ](https://en.wikipedia.org/wiki/AI_winter), потом оттепель, потом вторая волна разочарования (подробнее смотри предыдущий блок: [История машинного обучения](../1_history/index.html)).

![](.\images\ai1.jpg)

Пока в 2006 году Джеффри Хинтон, Руслан Салахутдинов, Осиндеро и Тех опубликовали работу про глубокую сеть доверия [«*A fast learning algorithm for deep belief nets (DBN)*»](https://www.cs.toronto.edu/~hinton/absps/fastnc.pdf), в которой они складывают несколько скрытых слоев, где нейроны внутри одного слоя не связаны друг с другом, но связаны с нейронами соседнего слоя (такой вид связей называется ограниченная машина Больцмана - *Restricted Boltzmann Machine* (RBM)) и называют их *Deep Belief Networks*. Данный подход обучения оказался весьма эффективным на большом количестве данных. А 2012 году свёрточная нейросеть [обошла всех в конкурсе ImageNet](https://habr.com/post/183380/), из-за чего в мире *внезапно вспомнили* о методах глубокого обучения, описанных еще в 90-х годах.

![](.\images\DBN.png)

Отличие **глубокого обучения** от классических нейронных сетей было в новых методах обучения, которые справлялись с **большими размерами сетей**, состоящая по крайней мере **из трех скрытых слоев**. Однако на сегодня это разделение условно, какое обучение можно считать глубоким, а какое не очень. Как правило, просто используют популярные «глубокие» библиотеки типа [*Keras*](https://keras.io/), [*TensorFlow*](https://github.com/tensorflow/tensorflow) и [*PyTorch*](https://pytorch.org/) даже когда необходимо собрать мини-сетку на пять слоёв. Просто потому что они удобнее всего того, что было раньше. И называют это просто нейросетями.



#### Разновидности архитектур нейронных сетей

![](.\images\NeuralNetworkZo19High.png)

Составить полный список топологий практически невозможно, так как новые появляются постоянно. Поэтому необходимо создать представление о базовых архитектурах искусственного интеллекта и не считать его исчерпывающим, особенно спустя время.

У изображения нейросетей в виде графов есть один недостаток: граф не покажет, как сеть работает. Например, вариационный автоэнкодер (*variational autoencoders*, VAE) выглядит в точности как простой автоэнкодер (AE), в то время как процесс обучения у этих нейросетей совершенно разный. А сферы применения различаются еще сильнее: в VAE на вход подается шум, из которого они получают новый вектор, в то время как AE просто находят для входных данных ближайший соответствующий вектор из тех, что они «запомнили».

Также следует отметить, что в виду новизны данного направления, не всегда общепринятые слова и сокращения обозначают одно и тоже. Под RNN иногда понимают рекурсивные нейронные сети (*recursive neural networks*), но обычно эта аббревиатура означает рекуррентную нейронную сеть (*recurrent neural network*). Во многих источниках RNN еще несет обозначение для любой рекуррентной архитектуры, включая LSTM, GRU и двунапраленные варианты. Иногда похожая путаница происходит с AE: VAE, DAE и им подобные могут называть просто AE. Многие сокращения содержат разное количество N в конце: можно сказать «сверточная нейронная сеть» — CNN (*Convolutional Neural Network*), а можно и просто «сверточная сеть» — CN.

Так среди базовых архитектур, как правило, выделяют свёрточные нейросети (CNN) и рекуррентные нейросети (RNN). Другие 



##### Свёрточные нейронные сети

Свёрточные сети сейчас на пике популярности. В основном они используются для поиска объектов на фото и видео, распознавания лиц, переноса стиля, генерации и дорисовки изображений, создания эффектов типа слоу-мо и улучшения качества фотографий. Сегодня CNN применяют везде, где есть картинки или видео. К примеру, даже на ваших телефонах несколько подобных сетей распознают объекты на фотографиях.

![](.\images\7rz.jpg)

> Картинка выше — результат работы open-source библиотеки [Detectron](https://github.com/facebookresearch/Detectron), от Facebook.

Проблема с изображениями всегда была в том, что непонятно, как выделять на них признаки. Текст можно разбить по предложениям, взять свойства слов из словарей. Картинки же приходилось размечать руками, объясняя машине, где у котика на фотографии ушки, а где хвост. Такой подход даже назвали «*handcrafting* признаков» и раньше все так и делали.

![](.\images\7s3.jpg)

Ручной крафтинг был крайне трудоемким и мало эффективным, во-первых, если котик на фотографии прижал ушки или отвернулся, то нейросеть ничего не увидит, а во-вторых, по примеру человеческого взгляда, мы не смотрим только на форму ушей и количество лап, а оцениваем объект по множеству разных признаков, о которых сами даже не задумываемся. А значит, не понимаем и не можем объяснить нейронной сети (алгоритму) какие признаки важны.

Таким образом, был сформулирован подход, при котором нейронная сеть  должна сама учиться распознавать и искать эти признаки. Для начала, необходимо выделить базовые признаки, составляя из каких-то базовых линий. Разделим изображение на блоки 8x8 пикселей и выберем какая линия доминирует в каждом из блоков — горизонтальная [-], вертикальная [|] или одна из диагональных [/]. Может быть и так, когда сразу несколько доминирует. В результате мы получим несколько массивов палочек, которые по сути являются простейшими признаками наличия очертаний объектов на картинке. По сути это тоже картинки, просто из палочек. Значит мы можем вновь выбрать блок 8x8 и посмотреть уже, как эти палочки сочетаются друг с другом. А потом еще и еще раз. Такая операция называется свёрткой, откуда и пошло название метода. Свёртку можно представить как слой нейросети, ведь нейрон — абсолютно любая функция.

![](.\images\7sc.jpg)

Когда мы прогоняем через нашу нейросеть кучу фотографий котов, она автоматически расставляет большие веса тем сочетаниям из палочек, которые увидела чаще всего. Причём неважно, это прямая линия спины или сложный геометрический объект типа мордочки — что-то обязательно будет ярко активироваться. На выходе же мы поставим простой перцептрон, который будет смотреть какие сочетания активировались и говорить кому они больше характерны — кошке или собаке.

![](.\images\CNN.png)

Красота идеи в том, что у нас получилась нейросеть, которая сама находит характерные признаки объектов. Больше нет необходимости отбирать их руками. Достаточно загружать миллион изображений с конкретным объектом, из открытых источников и сеть сама составит карты признаков из палочек и научится определять их на новых данных.

![](.\images\74s.jpg)



##### Рекуррентные нейронные сети

Вторая по популярности архитектура на сегодняшний день. Благодаря рекуррентным сетям у нас есть такие полезные вещи, как машинный перевод текстов и компьютерный синтез речи. На них решают все задачи, связанные с последовательностями — голосовые, текстовые или музыкальные.

Текст, речь или музыка — это последовательности. Каждое слово или звук — как бы самостоятельная единица, но которая зависит от предыдущих. Таким образом, становится достаточно легко обучить сеть произносить отдельные слова или буквы. Для этого берем множество размеченных на слова аудиофайлов и обучаем по входному слову выдавать нам последовательность сигналов, похожих на его произношение. Сравниваем с оригиналом от диктора и пытаемся максимально приблизиться к идеалу. Для такого подойдёт даже перцептрон.

![](.\images\RNN.png)

Вот только перцептрон не запоминает что он генерировал ранее и не получается последовательность. Для него каждый запуск как в первый раз. Появилась идея добавить к каждому нейрону память. Так были придуманы рекуррентные сети, в которых каждый нейрон запоминал все свои предыдущие ответы и при следующем запуске использовал их как дополнительный вход. То есть нейрон мог сказать самому себе в будущем, что следующий звук должен звучать повыше, а тут гласная была (очень упрощенный пример).

![](.\images\7s4.jpg)

Но была лишь одна проблема — когда каждый нейрон запоминал все прошлые результаты, в сети накапливалось большое количество входов и обучить такое количество связей становилось нереально. В подобной ситуации человек забывает старое и невостребованное в пользу новых и актуальных знаний, так и у нейросетей. Когда нейросеть не умеет забывать — её нельзя обучить.

![](.\images\LSTM.png)

В результате, сначала проблему решили, через ограничение памяти для  каждого нейрона. Но потом придумали в качестве этой «памяти» использовать специальные ячейки, похожие на память компьютера или регистры процессора. Каждая ячейка позволяет записывать в себя чисоловое значение, прочитать или сбросить — их назвали ячейки долгой и краткосрочной памяти (LSTM).

Когда нейрону необходимо сделать самому себе напоминание на будущее — он писал это в ячейку, когда наоборот вся история становилась ненужной (предложение, например, закончилось) — ячейки сбрасывались, оставляя только «долгосрочные» связи, как в классическом перцептроне. Другими словами, сеть обучалась не только устанавливать текущие связи, но и ставить напоминания.

Ярким и одним из первых примером применения CNN + RNN, является фейковые видеозаписи выступлений Обамы, для которой весьма неплохо была обучена нейросеть разговаривать его голосом, из массы открытых источников с его выступлениями. На этом примере видно, что имитировать голос — достаточно простая задача для сегодняшних машин. С видео требуется больше мощностей, но это пока.

> https://youtu.be/cQ54GDm1eL0 - You Won’t Believe What Obama Says In This Video! 





### Выводы

Все что выше, это сильное упрощение и всё-таки машинное обучение — это сложно. Для практического применения необходимо владение довольно сложной линейной алгеброй и ... . От этого, увы, никуда не деться. Область машинного обучения настолько разрослась, что уследить за всем почти невозможно. К примеру, ниже набор моделей для решения всего лишь одной задачи (детектирование объекта):

![](.\images\twbxrs3-fary0wir6wd-e4x_2_i.jpeg)



### Тест



---

### Источники

Машинное обучение для людей: https://vas3k.ru/blog/machine_learning/

Обзор разновидностей архитектур нейронных сетей: https://www.asimovinstitute.org/neural-network-zoo/ (ru: https://habr.com/ru/company/wunderfund/blog/313696/)



### Дополнительные материалы

Туториалы от Hugging Face с материалами для всех основных задач в NLP, CV и Audio: https://huggingface.co/tasks

Учебник по машинному обучению от Яндекс ШАД: https://ml-handbook.ru/

Сложность алгоритмов: https://www.bigocheatsheet.com/

Карта базовой математики для ML: https://app.learney.me/

Machine Learning mindmap: https://github.com/dformoso/machine-learning-mindmap/blob/master/Machine%20Learning.pdf

Пять трендов в DL, с исследованиями (с 2021 года) и прогноз: https://ai.googleblog.com/2022/01/google-research-themes-from-2021-and.html 

6 Roadmaps in One Place (AI, ML, Data Engineer, DL, DS, Big Data Engineer): https://i.am.ai/roadmap/

