## История машинного обучения

[TOC]

### Терминология

![ml hype curve](./images/hype.png)

> Актуальные [данные](https://trends.google.ru/trends/explore?date=all&q=Machine%20Learning) на Google Trends.

Искусственный интеллект, машинное обучение и глубокое обучение уже сейчас являются неотъемлемой частью многих предприятий — от достижений в области беспилотных транспортных средств и способности обыгрывать человека в такие игры, как [покер](http://www.techrepublic.com/article/ai-just-beat-the-worlds-4-best-poker-players-what-it-means/) и Го, к автоматизированному обслуживанию клиентов. Часто эти термины используются как синонимы, но, на самом деле, между ними есть различия. 



#### Искусственный интеллект

Термин  искусственный интеллект — широкое понятие, касающееся передового машинного интеллекта. В 1956 году на конференции по искусственному интеллекту в Дартмуте эта технология была описана следующим образом: «Каждый аспект обучения или любая другая особенность интеллекта могут быть в принципе так точно описаны, что машина сможет сымитировать их». К искусственному интеллекту можно относить — от компьютерных программ для игры в шахматы до систем распознавания речи, таких, например, как голосовой помощник [Amazon Alexa](http://www.techrepublic.com/article/amazon-alexa-the-smart-persons-guide/), способный воспринимать речь и отвечать на вопросы. В целом системы искусственного интеллекта можно разделить на три группы: ограниченный искусственный интеллект (*Narrow AI*), общий искусственный интеллект (*AGI*) и сверхразумный искусственный интеллект.

![](.\images\ii_2.png)

К ограниченному искусственному интеллекту, т.е. способных решать только одну конкретную задачу, относятся такие программы,  как [*Deep Blue*](http://www.techrepublic.com/article/ibm-watson-the-inside-story-of-how-the-jeopardy-winning-supercomputer-was-born-and-what-it-wants-to-do-next/) компании IBM, которая в 1996 году обыграла в шахматы Гарри Каспарова, или программа [*AlphaGo*](http://www.techrepublic.com/article/google-deepmind-the-smart-persons-guide/) компании *Google DeepMind*, которая в 2016 году обыграла чемпиона мира по Го Ли Седоля. 

Общий искусственный интеллект (*AGI*), стоит на одном уровне с человеческим интеллектом и может выполнять много разных задач.

А сверхразумный искусственный интеллект стоит на ступень выше человеческого. [Ник Бостром](http://www.techrepublic.com/article/evolution-to-ai-will-be-more-radical-than-ape-to-human-says-nick-bostrom/) описывает его следующим образом: это «интеллект, который намного умнее, чем лучший человеческий мозг, практически во всех областях, в том числе в научном творчестве, общей мудрости и социальных навыках». Другими словами, это когда машины станут намного умнее нас.



#### Машинное обучение

Машинное обучение является одним из направлений искусственного интеллекта. Основной принцип заключается в том, что машины получают данные и «обучаются» на них. Системы машинного обучения позволяют быстро применять знания, полученные при обучении на больших наборах данных, что позволяет им преуспевать в таких задачах, как распознавание лиц, распознавание речи, распознавание объектов, перевод, и многих других. В отличие от программ с закодированными вручную инструкциями для выполнения конкретных задач, машинное обучение позволяет системе научиться самостоятельно распознавать шаблоны и делать прогнозы.

В то время, как обе программы — и *Deep Blue*, и *DeepMind*, являются примерами использования искусственного интеллекта, Deep Blue была построена на заранее запрограммированном наборе правил, так что она никак не связана с машинным обучением. С другой стороны, DeepMind является примером машинного обучения: программа обыграла чемпиона мира по Го, обучая себя на большом наборе данных ходов, сделанных опытными игроками.



#### Глубокое обучение

Глубокое обучение является подмножеством машинного обучения. Оно использует некоторые методы машинного обучения для решения реальных задач, используя нейронные сетей, которые могут имитировать человеческое принятие решений. Глубокое обучение может быть дорогостоящим и требует огромных массивов данных для обучения. Это объясняется тем, что существует огромное количество параметров, которые необходимо настроить для алгоритмов обучения, чтобы избежать ложных срабатываний. Например, алгоритму глубокого обучения может быть дано указание «узнать», как выглядит кошка. Чтобы произвести обучение, потребуется огромное количество изображений для того, чтобы научиться различать мельчайшие детали, которые позволяют отличить кошку от, гепарда или пантеры, или лисицы.

Как уже упоминалось выше, в марте 2016 года искусственным интеллектом была достигнута крупная победа, когда программа *AlphaGo DeepMind* обыграла чемпиона мира по Го Ли Седоля в 4 из 5 игр с использованием глубокого обучения. Как объясняют в Google, система глубокого обучения работала путем комбинирования «[метода Монте-Карло для поиска в дереве](https://en.wikipedia.org/wiki/Monte_Carlo_tree_search) с глубокими нейронными сетями, которые прошли обучение с учителем на играх профессионалов и обучения с подкреплением на играх с собой».

![](.\images\mashinnoe-obuchenie-dlya-nespecialistov.jpg)

![](.\images\uf0t6gyvgn4ooh_14jda1locmmq.jpeg)

<img src=".\images\ai-ml-ds.png" style="zoom:110%;" />



### Вехи развития машинного обучения

![](.\images\AI_History.png)

> [Диаграмма](./images/AI History.svg) в высоком качестве.

Как уже было сказано выше, глубокое обучение уходит своими корнями в 1950-е годы, когда были созданы первые искусственные нейронные сети, основой для которых служили упрощенные модели биологических нейронов. Среди этих моделей наибольшей популярностью пользовалась система на основе **перцептрона**, предложенная Френком Розенблаттом. Будучи подключенной к «глазу» в виде фото-элементов, она могла обучаться распознаванию различные типов объектов.

Интерес к модели **многослойного перцептрона** (*multilayer perceptron* — *MLP*) продлился до 1969 года, когда Марвин Минский и Сеймур Пейперт выпустили книгу *Perceptrons* (*MIT Press*). В ней доказывалось, что линейный перцептрон неспособен классифицировать поведение нелинейной функции (*XOR* — исключающее ИЛИ). Публикация привела к сокращению инвестиций в исследования моделей нейронных сетей, пока в 1980-х не появилось новое поколение исследователей.

Рост вычислительных мощностей и развитие **метода обратного распространения ошибки** (известного в различных формах еще с 1960-х годов, но почти не применявшегося до 1980-х) стимулировал возрождение интереса к нейронным сетям. Первые **свёрточные нейронные сети** объединили в себе эти возможности с моделью визуального распознавания (на основе имевшихся к тому времени сведений о строении мозга млекопитающих) сложных изображений, как рукописные цифры и лица. В результате за счет применения одной и той же «подсети» к различным участкам изображения и агрегирования результатов в **высокоуровневые признаки** и был достигнут поразительный результат, для того времени.

В 1990-х — начало 2000-х годов интерес к нейронным сетям вновь спал, поскольку популярными стали более понятные модели и методы (на подобии метода опорных векторов и деревьев решений). В компьютерном зрении стало популярным **конструирование признаков** (*feature engineering*) — создание детекторов признаков для небольших элементов изображения и последующее их объединение вручную в нечто, что позволяет распознавать более сложные формы. Позже выяснится, что **сети глубокого обучения** учатся распознавать аналогичные признаки и объединять их похожим способом.

В конце 2000-х, с появлением мощных **графических процессоров** (*GPU*) стало возможным использовать параллельное обучение для большого числа нейронов в нейронной сети. 

Другим немаловажным фактором стала доступность Интернета и накопившиеся большие тренировочные наборы данных. Если раньше исследователи тренировали свои классификаторы на тысячах изображений, то теперь количество доступных изображений исчисляется десятками и сотнями миллионов. 

![](.\images\8795b2b383896c1d6724ffe910e83864.jpg)



#### Становление искусственного интеллекта (1943-1952)

**1943**: Первая работа, в которой была предложена модель искусственных нейронов, от Warren McCulloch и Walter Pits в 1943 году.

**1949**: Donald Hebb продемонстрировал теорию об изменениях силы связи между нейронами в результате постоянно повторяющейся стимуляции. Его правило теперь называется [*Hebbian learning*](https://ru.wikipedia.org/wiki/%D0%A2%D0%B5%D0%BE%D1%80%D0%B8%D1%8F_%D0%A5%D0%B5%D0%B1%D0%B1%D0%B0) или теория Хебба о нейронных (клеточных) ансамблях.

**1950**: Алан Тьюринг публикует работу «Вычислительные машины и интеллект», в которой он предложил проверять способность программ демонстрировать интеллектуальное поведение, эквивалентное человеческому интеллекту, впоследствии называнным тестом Тьюринга.



#### Рождение искусственного интеллекта (1952-1956)

**1955**: Аллен Ньюэлл и Герберт А. Саймон создали «первую программу искусственного интеллекта», которая была названа «*Logic Theorist* — теоретик логики». Эта программа доказала 38 из 52 математических теорем, а также нашла новые и более элегантные доказательства некоторых теорем.

**1956**: Термин «Artificial Intelligence — искусственный интеллект» впервые, в академической среде, был употреблен ученым Джон Маккарти на Дартмутской конференции. 

В этот период были созданы языки программирования высокого уровня, такие как *FORTRAN*, *LISP* и *COBOL*.



#### Золотые годы - ранний энтузиазм (1956-1974)

**1966**: Исследователи сделали упор на разработку алгоритмов, которые могут решать математические задачи. Джозеф Вейзенбаум создал первого чат-бота, который был назван *ELIZA*.

**1972**: в Японии был создан первый интеллектуальный человекоподобный робот, получивший название *WABOT-1*.



#### Первая зима искусственного интеллекта (1974-1980)

Нехватка финансирования со стороны правительств для исследований в области ИИ. Низкая производительность оборудования.

Интерес общественности к искусственному интеллекту был снижен.



#### Бум искусственного интеллекта (1980-1987)

**1980**: После зимнего периода ИИ вернулся с «экспертной системой». Экспертные системы были запрограммированы так, чтобы имитировать способность человека-эксперта принимать решения.

**1980**: в Стэнфордском университете состоялась первая национальная конференция Американской ассоциации искусственного интеллекта.



#### Вторая зима искусственного интеллекта (1987-1993)

Инвесторы и правительства снова прекратили финансирования исследований искусственного интеллекта из-за высокой стоимости и неэффективного результата. 

Экспертная система *XCON* предназначенная для достижения конкретных целей при решении задач с большим числом переменных, доказала эффективность с точки зрения затрат.



#### Формирование рынка (1993-2011)

**1997**: *Deep Blue* от *IBM* обыграл чемпиона мира по шахматам Гарри Каспарова.

**2002**: впервые ИИ вошел в дом в виде робота-пылесоса *Roomba*.

**2006**: компании, *Facebook*, *Twitter* и *Netflix*, начали использовать ИИ.



#### Deep learning, big data и AI (2011-...)

**2011**:  *Watson* от *IBM* победил в викторине «*Jeopardy*», где ему пришлось решать сложные вопросы и загадки. Watson доказал, что он понимает естественный язык и может быстро решать сложные вопросы.

**2012**: *Google* запустил функцию приложения для Android «*Google now*», которая могла предоставлять пользователю информацию в виде предсказаний.

**2014**: Чат-бот «*Eugene Goostman*» выиграл соревнование в известном «тесте Тьюринга».

**2018**: «*Project Debater*» от IBM провел дебаты на сложные темы с двумя профессиональными ораторами и показал отличные результаты.

**2019**: *Google* продемонстрировал виртуального помощника «*Duplex*», который записался к парикмахеру, а дама на другой стороне не заметила, что разговаривает с машиной.

Сейчас искусственный интеллект развился до поразительного уровня. Концепция глубокого обучения на больших данных и науки о данных переживает пик востребованности и популярности. В настоящее время такие компании, как *Google*, *Facebook*, *IBM* и *Amazon*, работают с ИИ и во многом задают направление всей новой индустрии.



#### Таблица

Ниже расширенная версия хронологии формирования и развития искусственного интеллекта, где отдельно помечены ключевые 💡 - открытия и 👷‍♂️- внедрения.

|      | Год        | Описание                                                     |
| :--: | ---------- | ------------------------------------------------------------ |
|  💡   | 1763, 1812 | [Bayes Theorem](https://wikipedia.org/wiki/Bayes%27_theorem) and its predecessors. This theorem and its applications underlie inference, describing the probability of an event occurring based on prior knowledge. |
|  💡   | 1805       | [Least Square Theory](https://wikipedia.org/wiki/Least_squares) by French mathematician Adrien-Marie Legendre. This theory, which you will learn about  in our Regression unit, helps in data fitting. |
|  💡   | 1913       | [Markov Chains](https://wikipedia.org/wiki/Markov_chain) named after Russian mathematician Andrey Markov is used to describe a sequence of possible events based on a previous state. |
|  💡   | 1943       | Впервые сформулирована математическая модель нейрона. Electronic brain by McCulloch and Pitts |
|      | 1950       | [Тест Тьюринга](https://zen.yandex.ru/media/id/5a20825dad0f22233a285e05/test-tiuringa-razumnost-poznaetsia-v-obscenii-5a83ffe2256d5c8bcd782434), в котором программа своими ответами должен убедить собеседника в своей человечности, что свидетельствует о способности  мыслить и наличие разумности. |
|  👷‍♂️  | 1952       | Создана одна из первых программ игры в шашки.                |
|      | 1956       | Впервые использован термин «искусственный интеллект (Artificial Intelligence)» ученым Джоном Маккарти на Дартмутской конференции. |
|  💡   | 1957       | [Perceptron](https://wikipedia.org/wiki/Perceptron) is a type of linear classifier invented by American psychologist Frank Rosenblatt that underlies advances in deep learning. |
|      | 1958       | LISP                                                         |
|      | 1959       | Решатель общих проблем (General Problem Solver) от Newell, Simon и Shaw. |
|      | 1960       | ADALINE by Widorow and Hoff - первая функция стоимости.      |
|  👷‍♂️  | 1964       | Joseph Weizenbaum создал первого чат-бота ELIZA.             |
|  💡   | 1964       | Универсальный Байесовский метод.                             |
|      | 1965       |                                                              |
|      | 1965       |                                                              |
|      | 1966       |                                                              |
|      | 1966       |                                                              |
|  💡   | 1967       | [Nearest Neighbor](https://wikipedia.org/wiki/Nearest_neighbor) is an algorithm originally designed to map routes. In an ML context it is used to  detect patterns. |
|      | 1968       |                                                              |
|      | 1969       | XOR problem - перцептрон не может выполнить исключающий ИЛИ. |
|      | 1970       | INTERNIST                                                    |
|      | 1970       | [Backpropagation](https://wikipedia.org/wiki/Backpropagation) is used to train [feedforward neural networks](https://wikipedia.org/wiki/Feedforward_neural_network). |
|      | 1972       | PROLOG                                                       |
|      | 1972       | Shakey                                                       |
|      | 1973       |                                                              |
|      | 1974       |                                                              |
|  💡   | 1979       | CNN                                                          |
|  💡   | 1982       | Bayesian Network                                             |
|  💡   | 1982       | RNN are artificial neural networks derived from feedforward neural networks that create temporal graphs. |
|      | 1983       | SOAP                                                         |
|  💡   | 1986       | Backpropagation                                              |
|      | 1987       |                                                              |
|      | 1989       | LeNet                                                        |
|      | 1991       | Учреждена ежегодная премия Лёбнера (AI Loebner), в рамках которой [искусственные интеллекты](https://robo-sapiens.ru/novosti/ii-nauchilsya-videt-myisli-cheloveka/) соревнуются в прохождении теста Тьюринга. |
|      | 1992       |                                                              |
|      | 1994       |                                                              |
|      | 1995       | MNIST                                                        |
|  💡   | 1995       | SVM                                                          |
|      | 1996       | DeepBlue vs Kasparov                                         |
|  💡   | 1997       | LSTM                                                         |
|  💡   | 2006       | Deep Boltzman Machine                                        |
|      | 2009       | ImageNet                                                     |
|      | 2010       | Создан [Kaggle](https://www.kaggle.com/) для организации конкурсов по исследованию данных. |
|  💡   | 2012       | AlexNet                                                      |
|  💡   | 2013       | Word2Vec                                                     |
|  💡   | 2014       | GAN                                                          |
|  👷‍♂️  | 2015       | Опубликована в свободный доступ библиотека [TensorFlow](https://www.tensorflow.org/), разработанная [Google Brain](https://ru.wikipedia.org/wiki/Google_Brain). |
|  👷‍♂️  | 2015       | Опубликован в свободный доступ библиотека [Keras](https://ru.wikipedia.org/wiki/Keras) для работы с фреймворком [TensorFlow](https://ru.wikipedia.org/wiki/TensorFlow). |
|      | 2015       |                                                              |
|      | 2015       | Чат-бот «Соня Гусева» имитирующий поведение 14-летней девочки  на русском языке прошел тест Тьюринга. |
|      | 2016       | Победа AlphaGo от DeepMind в игре Go с мировым чемпионом.    |
|  👷‍♂️  | 2016       | Опубликована в свободный доступ библиотека [PyTorch](https://pytorch.org/). |
|      | 2017       |                                                              |
|      | 2018       |                                                              |
|  👷‍♂️  | 2018       | MLOps                                                        |
|      | 2019       | Победа AlphaStar от DeepMind в игре Starcraft II.            |
|      | 2020       |                                                              |
|      | 2021       |                                                              |
|      | 2022       |                                                              |



### Технологический стек

![](.\images\Соты технологий.png)







#### Фреймворки

Такие фреймворки глубокого обучения, как *TensorFlow*, *Theano* и *Torch*, позволяют конструировать сложные нейронные сети, а библиотеки наподобие *Keras*, предоставляют еще больший уровень абстракции. 



#### MLOps

![](.\images\bf091c902ecac0f567e343f4989d6d4d.png)

MLOps (объединение технологий и процессов машинного обучения и подходов к внедрению разработанных моделей в бизнес-процессы) — это новый способ сотрудничества между представителями бизнеса, учеными, математиками, специалистами в области машинного обучения и IT-инженерами при создании систем искусственного интеллекта.

Цепочка разработки продукта начинается задолго до разработки модели. Ее первым шагом является определение задачи бизнеса, гипотезы о ценности, которую можно извлечь из данных, и бизнес-идеи по ее применению. 

Само понятие MLOps возникло как аналогия понятия DevOps применительно к моделям и технологиям машинного обучения. DevOps — это подход к разработке ПО, позволяющий повысить скорость внедрения отдельных изменений при сохранении гибкости и надежности с помощью ряда подходов, среди которых непрерывная разработка, разделение функций на ряд независимых микросервисов, автоматизированное тестирование и деплоймент отдельных изменений, глобальный мониторинг работоспособности, система оперативного реагирования на выявленные сбои и др. 

DevOps определил жизненный цикл программного обеспечения и в сообществе специалистов возникла идея использовать ту же методику применительно к большим данным. DataOps — попытка адаптировать и расширить методику с учетом особенностей хранения, передачи и обработки больших массивов данных в разнообразных и взаимодействующих друг с другом платформах.

С появлением определенной критической массы моделей машинного обучения, внедренных в бизнес-процессы предприятий, было замечено сильное сходство жизненного цикла математических моделей машинного обучения и жизненного цикла ПО. Разница только в том, что алгоритмы моделей создаются с помощью инструментов и методов машинного обучения. Поэтому естественным образом возникла идея применить и адаптировать для моделей машинного обучения уже известные подходы к разработке ПО. Таким образом, в жизненном цикле моделей машинного обучения можно выделить следующие ключевые этапы:

- определение бизнес-идеи;
- обучение модели;
- тестирование и внедрение модели в бизнес-процесс;
- эксплуатация модели.

Когда в процессе эксплуатации возникает необходимость изменить или дообучить модель на новых данных, цикл запускается заново — модель дорабатывается, тестируется, и деплоится новая версия.

> Почему дообучить, а не переобучить? Термин «переобучение модели» имеет двоякое толкование: среди специалистов он означает дефект модели, когда модель хорошо предсказывает, фактически повторяет прогнозируемый параметр на обучающей выборке, но гораздо хуже работает на внешней выборке данных. Естественно, такая модель является браком, поскольку данный дефект не позволяет ее применять.

![](.\images\data-sciene-lifecycle-model-flow.png)

![](.\images\Artificial-Intelligence-Machine-Learning-Data-Science-Pipeline.png)



### Вызовы



### Тест



---

### Источники

Пример организации MLOps цикла у Neoflex: https://www.neoflex.ru/solutions/neoflex-mlops-center

7 questions for DL: https://jameskle.com/writes/deep-learning-infrastructure-tooling

History of Artificial Intelligence: https://www.javatpoint.com/history-of-artificial-intelligence

https://habr.com/ru/post/401857/

Про MLOps: https://habr.com/ru/company/vtb/blog/508012/



### Дополнительные материалы

