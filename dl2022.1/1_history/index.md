## –ò—Å—Ç–æ—Ä–∏—è –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è

[TOC]

### –ú–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ

![ml hype curve](./images/hype.png)

–¢–µ—Ä–º–∏–Ω ¬´–∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç¬ª –±—ã–ª –≤–≤–µ–¥–µ–Ω –µ—â–µ –≤ 50-–µ –≥–æ–¥—ã –ø—Ä–æ—à–ª–æ–≥–æ –≤–µ–∫–∞. –ö –Ω–µ–º—É –æ—Ç–Ω–æ—Å–∏—Ç—Å—è –ª—é–±–∞—è –º–∞—à–∏–Ω–∞ –∏–ª–∏ –ø—Ä–æ–≥—Ä–∞–º–º–∞, –≤—ã–ø–æ–ª–Ω—è—é—â–∞—è –∑–∞–¥–∞—á–∏, ¬´–æ–±—ã—á–Ω–æ —Ç—Ä–µ–±—É—é—â–∏–µ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞ —á–µ–ª–æ–≤–µ–∫–∞¬ª. –°–æ –≤—Ä–µ–º–µ–Ω–µ–º –∫–æ–º–ø—å—é—Ç–µ—Ä—ã —Å–ø—Ä–∞–≤–ª—è–ª–∏—Å—å –≤—Å–µ —Å –Ω–æ–≤—ã–º–∏ –∏ –Ω–æ–≤—ã–º–∏ –∑–∞–¥–∞—á–∞–º–∏, –∫–æ—Ç–æ—Ä—ã–µ –ø—Ä–µ–∂–¥–µ —Ç—Ä–µ–±–æ–≤–∞–ª–∏ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞ —á–µ–ª–æ–≤–µ–∫–∞, —Ç–æ –µ—Å—Ç—å —Ç–æ, —á—Ç–æ –ø—Ä–µ–∂–¥–µ —Å—á–∏—Ç–∞–ª–æ—Å—å ¬´–∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–º –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–æ–º¬ª –ø–æ—Å—Ç–µ–ø–µ–Ω–Ω–æ –ø–µ—Ä–µ—Å—Ç–∞–ª–æ —Å –Ω–∏–º –∞—Å—Å–æ—Ü–∏–∏—Ä–æ–≤–∞—Ç—å—Å—è.

–ú–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ ‚Äî –æ–¥–∏–Ω –∏–∑ –º–µ—Ç–æ–¥–æ–≤ —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–π –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞, –∏ —Å –µ–≥–æ –ø–æ–º–æ—â—å—é –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –ø—Ä–æ–¥–≤–∏–Ω—É–ª—Å—è –≤–ø–µ—Ä–µ–¥. –ù–æ, —Ö–æ—Ç—è —ç—Ç–æ—Ç –º–µ—Ç–æ–¥ –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ –æ—á–µ–Ω—å –≤–∞–∂–µ–Ω, —ç—Ç–æ –¥–∞–ª–µ–∫–æ –Ω–µ –ø–µ—Ä–≤—ã–π –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–π —à–∞–≥ –≤ –∏—Å—Ç–æ—Ä–∏–∏ –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞: –∫–æ–≥–¥–∞-—Ç–æ –Ω–µ –º–µ–Ω–µ–µ –≤–∞–∂–Ω—ã–º–∏ –∫–∞–∑–∞–ª–∏—Å—å —ç–∫—Å–ø–µ—Ä—Ç–Ω—ã–µ —Å–∏—Å—Ç–µ–º—ã, –ª–æ–≥–∏—á–µ—Å–∫–∏–π –≤—ã–≤–æ–¥ –∏ –º–Ω–æ–≥–æ–µ –¥—Ä—É–≥–æ–µ.

–ó–∞ –ø–æ—Å–ª–µ–¥–Ω–µ–µ –¥–µ—Å—è—Ç–∏–ª–µ—Ç–∏–µ —Ç–µ—Ä–º–∏–Ω ¬´–º–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ¬ª —à–∏—Ä–æ–∫–æ —Ü–∏—Ç–∏—Ä—É–µ—Ç—Å—è –≤ –º–∞—Å—Å–º–µ–¥–∏–∞ –∏ –Ω–∞—É—á–Ω—ã—Ö —Ä–∞–±–æ—Ç–∞—Ö. –°—Ç–∞–ª –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è –≤ –æ—Ç–Ω–æ—à–µ–Ω–∏–∏ –±–æ–ª—å—à–æ–≥–æ —Å–ø–µ–∫—Ç—Ä–∞ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã—Ö –∫–ª–∞—Å—Å–æ–≤ –∑–∞–¥–∞—á.

> –ê–∫—Ç—É–∞–ª—å–Ω—ã–µ [–¥–∞–Ω–Ω—ã–µ](https://trends.google.ru/trends/explore?date=all&q=Machine%20Learning) –Ω–∞ Google Trends.

–î–ª—è —Ä–µ—à–µ–Ω–∏—è –∫–∞–∂–¥–æ–π –∑–∞–¥–∞—á–∏ —Å–æ–∑–¥–∞–µ—Ç—Å—è **–º–æ–¥–µ–ª—å**, —Ç–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–∏ —Å–ø–æ—Å–æ–±–Ω–∞—è –ø—Ä–∏–±–ª–∏–∑–∏—Ç—å—Å—è –∫ —á–µ–ª–æ–≤–µ—á–µ—Å–∫–æ–º—É —É—Ä–æ–≤–Ω—é —Ä–µ—à–µ–Ω–∏—è –¥–∞–Ω–Ω–æ–π –∑–∞–¥–∞—á–∏ –ø—Ä–∏ –ø—Ä–∞–≤–∏–ª—å–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏—è—Ö **–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤**. –û–±—É—á–µ–Ω–∏–µ —ç—Ç–æ–π –º–æ–¥–µ–ª–∏ ‚Äì —ç—Ç–æ –ø–æ—Å—Ç–æ—è–Ω–Ω–æ–µ –∏–∑–º–µ–Ω–µ–Ω–∏–µ –µ–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, —á—Ç–æ–±—ã –º–æ–¥–µ–ª—å –≤—ã–¥–∞–≤–∞–ª–∞ –≤—Å–µ –ª—É—á—à–∏–µ –∏ –ª—É—á—à–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã.

–†–∞–∑—É–º–µ–µ—Ç—Å—è, —ç—Ç–æ –ª–∏—à—å –æ–±—â–µ–µ –æ–ø–∏—Å–∞–Ω–∏–µ. –ö–∞–∫ –ø—Ä–∞–≤–∏–ª–æ, –≤—ã –Ω–µ –ø—Ä–∏–¥—É–º—ã–≤–∞–µ—Ç–µ –º–æ–¥–µ–ª—å —Å –Ω—É–ª—è, –∞ –ø–æ–ª—å–∑—É–µ—Ç–µ—Å—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –º–Ω–æ–≥–æ–ª–µ—Ç–Ω–∏—Ö –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π –≤ —ç—Ç–æ–π –æ–±–ª–∞—Å—Ç–∏, –ø–æ—Å–∫–æ–ª—å–∫—É —Å–æ–∑–¥–∞–Ω–∏–µ –Ω–æ–≤–æ–π –º–æ–¥–µ–ª–∏, –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è—â–µ–π —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ —Ö–æ—Ç—è –±—ã –Ω–∞ –æ–¥–Ω–æ–º –≤–∏–¥–µ –∑–∞–¥–∞—á ‚Äì —ç—Ç–æ –Ω–∞—Å—Ç–æ—è—â–µ–µ –Ω–∞—É—á–Ω–æ–µ –¥–æ—Å—Ç–∏–∂–µ–Ω–∏–µ. –ú–µ—Ç–æ–¥—ã –∑–∞–¥–∞–Ω–∏—è —Ü–µ–ª–µ–≤–æ–π —Ñ—É–Ω–∫—Ü–∏–∏, –æ–ø—Ä–µ–¥–µ–ª—è—é—â–µ–π, –Ω–∞—Å–∫–æ–ª—å–∫–æ —Ö–æ—Ä–æ—à–∏ –≤—ã–¥–∞–≤–∞–µ–º—ã–µ –º–æ–¥–µ–ª—å—é —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã (**—Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å**), —Ç–∞–∫–∂–µ –∑–∞–Ω–∏–º–∞—é—Ç —Ü–µ–ª—ã–µ —Ç–æ–º–∞ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π. –¢–æ –∂–µ —Å–∞–º–æ–µ –æ—Ç–Ω–æ—Å–∏—Ç—Å—è –∫ –º–µ—Ç–æ–¥–∞–º –∏–∑–º–µ–Ω–µ–Ω–∏—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –º–æ–¥–µ–ª–∏, —É—Å–∫–æ—Ä–µ–Ω–∏—è –æ–±—É—á–µ–Ω–∏—è –∏ –º–Ω–æ–≥–∏–º –¥—Ä—É–≥–∏–º. –î–∞–∂–µ –Ω–∞—á–∞–ª—å–Ω–∞—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —ç—Ç–∏—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –º–æ–∂–µ—Ç –∏–º–µ—Ç—å –±–æ–ª—å—à–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ!

–í –ø—Ä–æ—Ü–µ—Å—Å–µ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª—å —É—Å–≤–∞–∏–≤–∞–µ—Ç **–ø—Ä–∏–∑–Ω–∞–∫–∏**, –∫–æ—Ç–æ—Ä—ã–µ –º–æ–≥—É—Ç –æ–∫–∞–∑–∞—Ç—å—Å—è –≤–∞–∂–Ω—ã–º–∏ –¥–ª—è —Ä–µ—à–µ–Ω–∏—è –∑–∞–¥–∞—á–∏. –í—ã–¥–µ–ª–µ–Ω–∏–µ —Ç–∞–∫–∏—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∑–∞—á–∞—Å—Ç—É—é –Ω–µ –º–µ–Ω–µ–µ, –∞ –∏–Ω–æ–≥–¥–∞ –Ω–∞–º–Ω–æ–≥–æ –±–æ–ª–µ–µ —Ü–µ–Ω–Ω–æ, —á–µ–º —Ä–µ—à–µ–Ω–∏–µ –æ—Å–Ω–æ–≤–Ω–æ–π –∑–∞–¥–∞—á–∏.






![](.\images\mashinnoe-obuchenie-dlya-nespecialistov.jpg)



![](.\images\uf0t6gyvgn4ooh_14jda1locmmq.jpeg)



![](.\images\ai-ml-ds.png)



### –í–µ—Ö–∏ —Ä–∞–∑–≤–∏—Ç–∏—è –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è

![](.\images\AI History.png)

> [–î–∏–∞–≥—Ä–∞–º–º–∞](./images/AI History.svg) –≤ –≤—ã—Å–æ–∫–æ–º –∫–∞—á–µ—Å—Ç–≤–µ.

–ó–Ω–∞–º–µ–Ω–∞—Ç–µ–ª—å–Ω—ã–µ –æ—Ç–∫—Ä—ã—Ç–∏—è(—Ü–≤–µ—Ç - –∞–ª–≥–æ—Ä–∏—Ç–º—ã –∏ —Ç–µ–æ—Ä–µ–º—ã), —Å–æ–±—ã—Ç–∏—è(—Ü–≤–µ—Ç - –∫–æ–Ω—Ñ–µ—Ä–µ–Ω—Ü–∏–∏,)  –∏ –≤–Ω–µ–¥—Ä–µ–Ω–∏—è —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–π(—Ü–≤–µ—Ç - MLOps):

üí° - –æ—Ç–∫—Ä—ã—Ç–∏–µ, üì∫ - —Å–æ–±—ã—Ç–∏–µ, üë∑‚Äç‚ôÇÔ∏è- –≤–Ω–µ–¥—Ä–µ–Ω–∏–µ. 

|      |            |                                                              |
| :--: | ---------- | ------------------------------------------------------------ |
|  üí°   | 1763, 1812 | [Bayes Theorem](https://wikipedia.org/wiki/Bayes%27_theorem) and its predecessors. This theorem and its applications underlie inference, describing the probability of an event occurring based on prior knowledge. |
|  üí°   | 1805       | [Least Square Theory](https://wikipedia.org/wiki/Least_squares) by French mathematician Adrien-Marie Legendre. This theory, which you will learn about  in our Regression unit, helps in data fitting. |
|  üí°   | 1913       | [Markov Chains](https://wikipedia.org/wiki/Markov_chain) named after Russian mathematician Andrey Markov is used to describe a sequence of possible events based on a previous state. |
|      | 1943       |                                                              |
|      | 1950       |                                                              |
|      | 1952       |                                                              |
|  üì∫   | 1956       | Dartmouth Summer Research Project                            |
|      | 1957       |                                                              |
|  üë∑‚Äç‚ôÇÔ∏è  | 1958       | LISP                                                         |
|      | 1959       |                                                              |
|      | 1960       |                                                              |
|  üë∑‚Äç‚ôÇÔ∏è  | 1964       | ELIZA                                                        |
|      | 1964       |                                                              |
|      | 1965       |                                                              |
|      | 1965       |                                                              |
|      | 1966       |                                                              |
|      | 1966       |                                                              |
|  üí°   | 1967       | [Nearest Neighbor](https://wikipedia.org/wiki/Nearest_neighbor) is an algorithm originally designed to map routes. In an ML context it is used to  detect patterns. |
|      | 1968       |                                                              |
|      | 1969       |                                                              |
|      | 1970       |                                                              |
|      | 1970       |                                                              |
|      | 1972       |                                                              |
|      | 1972       |                                                              |
|      | 1973       |                                                              |
|      | 1974       |                                                              |
|      | 1979       |                                                              |
|      | 1982       |                                                              |
|      | 1983       |                                                              |
|      | 1986       |                                                              |
|      | 1987       |                                                              |
|      | 1989       |                                                              |
|      | 1992       |                                                              |
|      | 1994       |                                                              |
|      | 1995       |                                                              |
|  üí°   | 1995       | SVM                                                          |
|      | 1996       |                                                              |
|  üí°   | 1997       | LSTM                                                         |
|      | 2006       |                                                              |
|      | 2009       |                                                              |
|  üí°üì∫  | 2012       |                                                              |
|  üí°   | 2013       | Word2Vec                                                     |
|  üí°   | 2014       | GAN                                                          |
|  üë∑‚Äç‚ôÇÔ∏è  | 2015       | MLOps                                                        |
|      | 2016       |                                                              |
|      | 2017       |                                                              |
|      | 2018       |                                                              |
|      | 2019       |                                                              |
|      | 2020       |                                                              |
|      | 2021       |                                                              |



- üí°1763, 1812 [Bayes Theorem](https://wikipedia.org/wiki/Bayes%27_theorem) and its predecessors. This theorem and its applications underlie inference, describing the probability of an event occurring based on prior knowledge.
- üí°1805 [Least Square Theory](https://wikipedia.org/wiki/Least_squares) by French mathematician Adrien-Marie Legendre. This theory, which you will learn about  in our Regression unit, helps in data fitting.
- üí°1913 - [Markov Chains](https://wikipedia.org/wiki/Markov_chain) named after Russian mathematician Andrey Markov is used to describe a sequence of possible events based on a previous state.
- 1943
- 1950
- 1952
- 1956 Dartmouth Summer Research Project
- 1957 [Perceptron](https://wikipedia.org/wiki/Perceptron) is a type of linear classifier invented by American psychologist Frank Rosenblatt that underlies advances in deep learning.
- 1958 LISP
- 1959
- 1960
- 1964 ELIZA
- 1964
- 1965
- 1965
- 1966
- 1966
- 1967 [Nearest Neighbor](https://wikipedia.org/wiki/Nearest_neighbor) is an algorithm originally designed to map routes. In an ML context it is used to  detect patterns.
- 1968
- 1969
- 1970 INTERNIST
- 1970 [Backpropagation](https://wikipedia.org/wiki/Backpropagation) is used to train [feedforward neural networks](https://wikipedia.org/wiki/Feedforward_neural_network).
- 1972 PROLOG
- 1972 Shakey 
- 1973
- 1974
- 1979 CNN
- 1982 Bayesian Network
- 1982 [Recurrent Neural Networks](https://wikipedia.org/wiki/Recurrent_neural_network) are artificial neural networks derived from feedforward neural networks that create temporal graphs.
- 1983 SOAP
- 1986
- 1987
- 1989 LeNet
- 1992
- 1994
- 1995 SVM
- 1995
- 1996
- 1997 LSTM
- 2006
- 2009
- 2012
- 2013 Word2Vec
- 2014 GAN
- 2016
- 2015 MLOps
- 2017
- 2018
- 2019
- 2020
- 2021
- 2022?



### –¢–µ—Ö–Ω–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–π —Å—Ç–µ–∫

#### –§—Ä–µ–π–º–≤–æ—Ä–∫–∏

#### MLOps

### –í—ã–∑–æ–≤—ã



---

### –ò—Å—Ç–æ—á–Ω–∏–∫–∏

–ü—Ä–∏–º–µ—Ä –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏ MLOps —Ü–∏–∫–ª–∞ —É Neoflex: https://www.neoflex.ru/solutions/neoflex-mlops-center

7 questions for DL: https://jameskle.com/writes/deep-learning-infrastructure-tooling



### –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –º–∞—Ç–µ—Ä–∏–∞–ª—ã

