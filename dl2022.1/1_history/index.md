## История машинного обучения

[TOC]

### Машинное обучение

![ml hype curve](./images/hype.png)

Термин «искусственный интеллект» был введен еще в 50-е годы прошлого века. К нему относится любая машина или программа, выполняющая задачи, «обычно требующие интеллекта человека». Со временем компьютеры справлялись все с новыми и новыми задачами, которые прежде требовали интеллекта человека, то есть то, что прежде считалось «искусственным интеллектом» постепенно перестало с ним ассоциироваться.

Машинное обучение — один из методов реализации приложений искусственного интеллекта, и с его помощью искусственный интеллект значительно продвинулся вперед. Но, хотя этот метод действительно очень важен, это далеко не первый значительный шаг в истории искусственного интеллекта: когда-то не менее важными казались экспертные системы, логический вывод и многое другое.

За последнее десятилетие термин «машинное обучение» широко цитируется в массмедиа и научных работах. Стал использоваться в отношении большого спектра разнообразных классов задач.

> Актуальные [данные](https://trends.google.ru/trends/explore?date=all&q=Machine%20Learning) на Google Trends.

Для решения каждой задачи создается **модель**, теоретически способная приблизиться к человеческому уровню решения данной задачи при правильных значениях **параметров**. Обучение этой модели – это постоянное изменение ее параметров, чтобы модель выдавала все лучшие и лучшие результаты.

Разумеется, это лишь общее описание. Как правило, вы не придумываете модель с нуля, а пользуетесь результатами многолетних исследований в этой области, поскольку создание новой модели, превосходящей существующие хотя бы на одном виде задач – это настоящее научное достижение. Методы задания целевой функции, определяющей, насколько хороши выдаваемые моделью результаты (**функции потерь**), также занимают целые тома исследований. То же самое относится к методам изменения параметров модели, ускорения обучения и многим другим. Даже начальная инициализация этих параметров может иметь большое значение!

В процессе обучения модель усваивает **признаки**, которые могут оказаться важными для решения задачи. Выделение таких признаков зачастую не менее, а иногда намного более ценно, чем решение основной задачи.






![](.\images\mashinnoe-obuchenie-dlya-nespecialistov.jpg)



![](.\images\uf0t6gyvgn4ooh_14jda1locmmq.jpeg)



![](.\images\ai-ml-ds.png)



### Вехи развития машинного обучения

![](.\images\AI_History.png)

> [Диаграмма](./images/AI History.svg) в высоком качестве.

Машинное обучение и глубокое обучение уходит своими корнями в 1950-е годы, когда были созданы первые искусственные нейронные сети, основой для которых служили упрощенные модели биологических нейронов. Среди этих моделей наибольшей популярностью пользовалась система на основе **перцептрона**, предложенная Френком Розенблаттом. Будучи подключенной к «глазу» в виде фото-элементов, она могла обучаться распознаванию различные типов объектов.

Интерес к модели **многослойного перцептрона** (*multilayer perceptron* — *MLP*) продлился до 1969 года, когда Марвин Минский и Сеймур Пейперт выпустили книгу *Perceptrons* (*MIT Press*). В ней доказывалось, что линейный перцептрон неспособен классифицировать поведение нелинейной функции (*XOR* — исключающее ИЛИ). Публикация привела к сокращению инвестиций в исследования моделей нейронных сетей, пока в 1980-х не появилось новое поколение исследователей.

Рост вычислительных мощностей и развитие **метода обратного распространения ошибки** (известного в различных формах еще с 1960-х годов, но почти не применявшегося до 1980-х) стимулировал возрождение интереса к нейронным сетям. Первые **свёрточные нейронные сети** объединили в себе эти возможности с моделью визуального распознавания (на основе имевшихся к тому времени сведений о строении мозга млекопитающих) сложных изображений, как рукописные цифры и лица. В результате за счет применения одной и той же «подсети» к различным участкам изображения и агрегирования результатов в **высокоуровневые признаки** и был достигнут поразительный результат, для того времени.

В 1990-х — начало 2000-х годов интерес к нейронным сетям вновь спал, поскольку популярными стали более понятные модели и методы (на подобии метода опорных векторов и деревьев решений). В компьютерном зрении стало популярным **конструирование признаков** (*feature engineering*) — создание детекторов признаков для небольших элементов изображения и последующее их объединение вручную в нечто, что позволяет распознавать более сложные формы. Позже выяснится, что **сети глубокого обучения** учатся распознавать аналогичные признаки и объединять их похожим способом.

В конце 2000-х, с появлением мощных **графических процессоров** (*GPU*) стало возможным использовать параллельное обучение для большого числа нейронов в нейронной сети. 

Другим немаловажным фактором стала доступность Интернета и накопившиеся большие тренировочные наборы данных. Если раньше исследователи тренировали свои классификаторы на тысячах изображений, то теперь количество доступных изображений исчисляется десятками и сотнями миллионов. 



#### Становление искусственного интеллекта (1943-1952)

**1943**: Первая работа, в которой была предложена модель искусственных нейронов, от Warren McCulloch и Walter Pits в 1943 году.

**1949**: Donald Hebb продемонстрировал теорию об изменениях силы связи между нейронами в результате постоянно повторяющейся стимуляции. Его правило теперь называется [*Hebbian learning*](https://ru.wikipedia.org/wiki/%D0%A2%D0%B5%D0%BE%D1%80%D0%B8%D1%8F_%D0%A5%D0%B5%D0%B1%D0%B1%D0%B0) или теория Хебба о нейронных (клеточных) ансамблях.

**1950**: Алан Тьюринг публикует работу «Вычислительные машины и интеллект», в которой он предложил проверять способность программ демонстрировать интеллектуальное поведение, эквивалентное человеческому интеллекту, впоследствии называнным тестом Тьюринга.



#### Рождение искусственного интеллекта (1952-1956)

**1955**: Аллен Ньюэлл и Герберт А. Саймон создали «первую программу искусственного интеллекта», которая была названа «*Logic Theorist* — теоретик логики». Эта программа доказала 38 из 52 математических теорем, а также нашла новые и более элегантные доказательства некоторых теорем.

**1956**: Термин «Artificial Intelligence — искусственный интеллект» впервые, в академической среде, был употреблен ученым Джон Маккарти на Дартмутской конференции. 

В этот период были созданы языки программирования высокого уровня, такие как *FORTRAN*, *LISP* и *COBOL*.



#### Золотые годы - ранний энтузиазм (1956-1974)

**1966**: Исследователи сделали упор на разработку алгоритмов, которые могут решать математические задачи. Джозеф Вейзенбаум создал первого чат-бота, который был назван *ELIZA*.

**1972**: в Японии был создан первый интеллектуальный человекоподобный робот, получивший название *WABOT-1*.



#### Первая зима искусственного интеллекта (1974-1980)

Нехватка финансирования со стороны правительств для исследований в области ИИ. Низкая производительность оборудования.

Интерес общественности к искусственному интеллекту был снижен.



#### Бум искусственного интеллекта (1980-1987)

**1980**: После зимнего периода ИИ вернулся с «экспертной системой». Экспертные системы были запрограммированы так, чтобы имитировать способность человека-эксперта принимать решения.

**1980**: в Стэнфордском университете состоялась первая национальная конференция Американской ассоциации искусственного интеллекта.



#### Вторая зима искусственного интеллекта (1987-1993)

Инвесторы и правительства снова прекратили финансирования исследований искусственного интеллекта из-за высокой стоимости и неэффективного результата. 

Экспертная система *XCON* предназначенная для достижения конкретных целей при решении задач с большим числом переменных, доказала эффективность с точки зрения затрат.



#### Формирование рынка (1993-2011)

**1997**: *Deep Blue* от *IBM* обыграл чемпиона мира по шахматам Гарри Каспарова.

**2002**: впервые ИИ вошел в дом в виде робота-пылесоса *Roomba*.

**2006**: компании, *Facebook*, *Twitter* и *Netflix*, начали использовать ИИ.



#### Deep learning, big data и AI (2011-...)

**2011**:  *Watson* от *IBM* победил в викторине «*Jeopardy*», где ему пришлось решать сложные вопросы и загадки. Watson доказал, что он понимает естественный язык и может быстро решать сложные вопросы.

**2012**: *Google* запустил функцию приложения для Android «*Google now*», которая могла предоставлять пользователю информацию в виде предсказаний.

**2014**: Чат-бот «*Eugene Goostman*» выиграл соревнование в известном «тесте Тьюринга».

**2018**: «*Project Debater*» от IBM провел дебаты на сложные темы с двумя профессиональными ораторами и показал отличные результаты.

**2019**: *Google* продемонстрировал виртуального помощника «*Duplex*», который записался к парикмахеру, а дама на другой стороне не заметила, что разговаривает с машиной.

Сейчас искусственный интеллект развился до поразительного уровня. Концепция глубокого обучения на больших данных и науки о данных переживает пик востребованности и популярности. В настоящее время такие компании, как *Google*, *Facebook*, *IBM* и *Amazon*, работают с ИИ и во многом задают направление всей новой индустрии.



#### Таблица

Ниже расширенная версия хронологии формирования и развития искусственного интеллекта, где отдельно помечены ключевые 💡 - открытия и 👷‍♂️- внедрения.

|      | Год        | Описание                                                     |
| :--: | ---------- | ------------------------------------------------------------ |
|  💡   | 1763, 1812 | [Bayes Theorem](https://wikipedia.org/wiki/Bayes%27_theorem) and its predecessors. This theorem and its applications underlie inference, describing the probability of an event occurring based on prior knowledge. |
|  💡   | 1805       | [Least Square Theory](https://wikipedia.org/wiki/Least_squares) by French mathematician Adrien-Marie Legendre. This theory, which you will learn about  in our Regression unit, helps in data fitting. |
|  💡   | 1913       | [Markov Chains](https://wikipedia.org/wiki/Markov_chain) named after Russian mathematician Andrey Markov is used to describe a sequence of possible events based on a previous state. |
|  💡   | 1943       | Впервые сформулирована математическая модель нейрона. Electronic brain by McCulloch and Pitts |
|      | 1950       | [Тест Тьюринга](https://zen.yandex.ru/media/id/5a20825dad0f22233a285e05/test-tiuringa-razumnost-poznaetsia-v-obscenii-5a83ffe2256d5c8bcd782434), в котором программа своими ответами должен убедить собеседника в своей человечности, что свидетельствует о способности  мыслить и наличие разумности. |
|  👷‍♂️  | 1952       | Создана одна из первых программ игры в шашки.                |
|      | 1956       | Впервые использован термин «искусственный интеллект (Artificial Intelligence)» ученым Джоном Маккарти на Дартмутской конференции. |
|      | 1957       | [Perceptron](https://wikipedia.org/wiki/Perceptron) is a type of linear classifier invented by American psychologist Frank Rosenblatt that underlies advances in deep learning. |
|  👷‍♂️  | 1958       | LISP                                                         |
|      | 1959       |                                                              |
|      | 1960       |                                                              |
|  👷‍♂️  | 1964       | Joseph Weizenbaum создал первого чат-бота ELIZA.             |
|      | 1964       |                                                              |
|      | 1965       |                                                              |
|      | 1965       |                                                              |
|      | 1966       |                                                              |
|      | 1966       |                                                              |
|  💡   | 1967       | [Nearest Neighbor](https://wikipedia.org/wiki/Nearest_neighbor) is an algorithm originally designed to map routes. In an ML context it is used to  detect patterns. |
|      | 1968       |                                                              |
|      | 1969       |                                                              |
|      | 1970       | INTERNIST                                                    |
|      | 1970       | [Backpropagation](https://wikipedia.org/wiki/Backpropagation) is used to train [feedforward neural networks](https://wikipedia.org/wiki/Feedforward_neural_network). |
|      | 1972       | PROLOG                                                       |
|      | 1972       | Shakey                                                       |
|      | 1973       |                                                              |
|      | 1974       |                                                              |
|  💡   | 1979       | CNN                                                          |
|  💡   | 1982       | Bayesian Network                                             |
|  💡   | 1982       | RNN are artificial neural networks derived from feedforward neural networks that create temporal graphs. |
|      | 1983       | SOAP                                                         |
|      | 1986       |                                                              |
|      | 1987       |                                                              |
|      | 1989       | LeNet                                                        |
|      | 1991       | Учреждена ежегодная премия Лёбнера (AI Loebner), в рамках которой [искусственные интеллекты](https://robo-sapiens.ru/novosti/ii-nauchilsya-videt-myisli-cheloveka/) соревнуются в прохождении теста Тьюринга. |
|      | 1992       |                                                              |
|      | 1994       |                                                              |
|      | 1995       | MNIST                                                        |
|  💡   | 1995       | SVM                                                          |
|      | 1996       |                                                              |
|  💡   | 1997       | LSTM                                                         |
|      | 2006       |                                                              |
|      | 2009       |                                                              |
|      | 2010       | Создан Kaggle для организации конкурсов по исследованию данных. |
|  💡📺  | 2012       |                                                              |
|  💡   | 2013       | Word2Vec                                                     |
|  💡   | 2014       | GAN                                                          |
|      | 2015       | Опубликована в свободный доступ библиотека [TensorFlow](https://www.tensorflow.org/), разработанная [Google Brain](https://ru.wikipedia.org/wiki/Google_Brain). |
|  👷‍♂️  | 2015       | MLOps                                                        |
|      | 2015       | Чат-бот на русском языке прошел тест Тьюринга, написанный 14-летней Соней Гусевой из Петербурга. |
|      | 2016       | Победа AlphaGo от DeepMind в игре Go с мировым чемпионом.    |
|      | 2016       | Опубликована в свободный доступ библиотека [PyTorch](https://pytorch.org/). |
|      | 2017       |                                                              |
|      | 2018       |                                                              |
|      | 2019       | Победа AlphaStar от DeepMind в игре Starcraft II.            |
|      | 2020       |                                                              |
|      | 2021       |                                                              |



### Технологический стек

#### Фреймворки

#### MLOps

### Вызовы



---

### Источники

Пример организации MLOps цикла у Neoflex: https://www.neoflex.ru/solutions/neoflex-mlops-center

7 questions for DL: https://jameskle.com/writes/deep-learning-infrastructure-tooling

History of Artificial Intelligence: https://www.javatpoint.com/history-of-artificial-intelligence





### Дополнительные материалы

